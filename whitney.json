{
	"title": {
		"media": {
			"url": "https://images.theconversation.com/files/374303/original/file-20201210-18-elk4m.jpg?ixlib=rb-4.1.0&rect=0%2C22%2C7500%2C5591&q=45&auto=format&w=926&fit=clip",
			"caption": "Neural Network Evolution",
			"credit": "https://link.springer.com/article/10.1007/BF02478259"
		},
		"text": {
			"headline": "History of Neural Networks",
			"text": "<p>A timeline highlighting key breakthroughs in the development of neural networks, from early theoretical foundations to modern deep learning applications.</p>"
		}
	},
	"events": [
		{
			"media": {
				"url": "https://av-eks-lekhak.s3.amazonaws.com/media/__sized__/article_images/36bb334b-4393-4c4c-82a8-ecac23d4ac70-thumbnail_webp-600x300.webp",
				"caption": "McCulloch & Pitts Model of the Neuron",
				"credit": "Springer"
			},
			"start_date": {
				"year": "1943"
			},
			"text": {
				"headline": "McCulloch & Pitts Neural Model",
				"text": "<p>Warren McCulloch and Walter Pitts proposed the first mathematical model of artificial neurons, marking the birth of neural networks. (<a href='https://link.springer.com/article/10.1007/BF02478259'>Springer</a>)</p>"
			}
		},
		{
			"media": {
				"url": "https://d3d0lqu00lnqvz.cloudfront.net/donaldoldinghebb/donaldhebb.jpg",
				"caption": "Donald Hebb",
				"credit": "https://www.researchgate.net/publication/200772719_The_Organization_of_Behavior"
			},
			"start_date": {
				"year": "1949"
			},
			"text": {
				"headline": "Hebbian Learning",
				"text": "<p>Donald Hebb introduced the principle 'neurons that fire together, wire together,' laying the foundation for learning mechanisms in artificial networks. (<a href='https://www.researchgate.net/publication/200772719_The_Organization_of_Behavior'>ResearchGate</a>)</p>"
			}
		},
		{
			"media": {
				"url": "https://www.researchgate.net/profile/Eduardo-Alves-13/publication/226190708/figure/fig1/AS:393793066029064@1470898992097/Rosenblatts-perceptron.png",
				"caption": "The Perceptron",
				"credit": "https://psycnet.apa.org/record/1959-09865-001"
			},
			"start_date": {
				"year": "1958"
			},
			"text": {
				"headline": "Rosenblatt's Perceptron",
				"text": "<p>Frank Rosenblatt invented the Perceptron, the first trainable neural network, sparking optimism in machine learning. (<a href='https://psycnet.apa.org/record/1959-09865-001'>APA</a>)</p>"
			}
		},
		{
			"media": {
				"url": "https://miro.medium.com/v2/resize:fit:1000/1*p6ckKuaSJMchiVblO3PscA.png",
				"caption": "Minsky & Papert's Perceptrons",
				"credit": "https://mitpress.mit.edu/9780262630221/perceptrons/"
			},
			"start_date": {
				"year": "1969"
			},
			"text": {
				"headline": "AI Winter Triggered",
				"text": "<p>Minsky and Papert showed limitations of the Perceptron (e.g., XOR problem), leading to reduced funding and the first AI Winter. (<a href='https://mitpress.mit.edu/9780262630221/perceptrons/'>MIT Press</a>)</p>"
			}
		},
		{
			"media": {
				"url": "https://www.researchgate.net/profile/Kunihiko-Fukushima-2/publication/336163445/figure/fig1/AS:809198191398912@1569939291177/The-architecture-of-the-neocognitron.png",
				"caption": "Neocognitron Architecture",
				"credit": "Sprhttps://link.springer.com/article/10.1007/BF00344251"
			},
			"start_date": {
				"year": "1982"
			},
			"text": {
				"headline": "Fukushima's Neocognitron",
				"text": "<p>Kunihiko Fukushima developed the Neocognitron, a precursor to convolutional neural networks (CNNs). (<a href='https://link.springer.com/article/10.1007/BF00344251'>Springer</a>)</p>"
			}
		},
		{
			"media": {
				"url": "https://cdn.botpenguin.com/assets/website/image_347c8041fd.png",
				"caption": "Backpropagation Concept",
				"credit": "https://www.nature.com/articles/323533a0"
			},
			"start_date": {
				"year": "1986"
			},
			"text": {
				"headline": "Backpropagation Algorithm",
				"text": "<p>Rumelhart, Hinton, and Williams popularized backpropagation, reviving neural network research. (<a href='https://www.nature.com/articles/323533a0'>Nature</a>)</p>"
			}
		},
		{
			"media": {
				"url": "https://www.deep-mind.org/wp-content/uploads/2023/03/CoverPage_The-Universal-Approximation-Theorem.png",
				"caption": "Universal Approximation Theorem",
				"credit": "https://link.springer.com/article/10.1007/BF02551274"
			},
			"start_date": {
				"year": "1989"
			},
			"text": {
				"headline": "Universal Approximation Theorem",
				"text": "<p>George Cybenko proved that neural networks with one hidden layer can approximate any continuous function. (<a href='https://link.springer.com/article/10.1007/BF02551274'>Springer</a>)</p>"
			}
		},
		{
			"media": {
				"url": "https://neurohive.io/wp-content/uploads/2018/10/AlexNet-1.png",
				"caption": "AlexNet (2012) ImageNet Results",
				"credit": "https://www.cs.toronto.edu/~hinton/absps/imagenet.pdf"
			},
			"start_date": {
				"year": "2012"
			},
			"text": {
				"headline": "AlexNet and Deep Learning Boom",
				"text": "<p>Krizhevsky, Sutskever, and Hinton's AlexNet won ImageNet by a wide margin, sparking the deep learning revolution. (<a href='https://www.cs.toronto.edu/~hinton/absps/imagenet.pdf'>UofT</a>)</p>"
			}
		},
		{
			"media": {
				"url": "https://miro.medium.com/v2/resize:fit:1000/format:webp/1*BuwxI4GH1JpPtvPUMfzx5g.png",
				"caption": "ResNets and Transformers",
				"credit": "https://arxiv.org/abs/1706.03762"
			},
			"start_date": {
				"year": "2015"
			},
			"text": {
				"headline": "Modern Advances (ResNet & Transformers)",
				"text": "<p>Deep architectures like ResNet (2015) and Transformers (2017) enabled state-of-the-art performance in vision and language. (<a href='https://arxiv.org/abs/1512.03385'>ResNet</a>, <a href='https://arxiv.org/abs/1706.03762'>Transformers</a>)</p>"
			}
		}
	]
}
